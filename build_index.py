# build_index_fast.py
import os, json, hashlib, numpy as np
from typing import List, Dict, Any, Tuple

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
# before
# from langchain_community.embeddings import HuggingFaceEmbeddings
# after
from langchain_huggingface import HuggingFaceEmbeddings

try:
    from langchain.schema import Document
except Exception:
    from langchain_core.documents import Document

IN_JSONL   = "./artifacts/raw_docs.jsonl"
INDEX_DIR  = "./artifacts/faiss_index"
CACHE_NPZ  = "./artifacts/emb_cache.npz"  # md5 -> vector ìºì‹œ
EMB_MODEL  = "intfloat/multilingual-e5-base" # "paraphrase-multilingual-MiniLM-L12-v2"  # "intfloat/multilingual-e5-base"  # ë˜ëŠ” "BAAI/bge-m3"

CHUNK_SIZE = 555
CHUNK_OVER = 55
BATCH_SIZE = 32  # í¬ê²Œ! (MPS/CPUì— ë§ì¶° ì¡°ì ˆ)
NORMALIZE  = True # ì½”ì‚¬ì¸ ê²€ìƒ‰ ì•ˆì •í™”

def autodetect_device() -> str:
    # torchê°€ ê¹”ë ¤ìˆê³  MPS ê°€ëŠ¥í•˜ë©´ 'mps', ì•„ë‹ˆë©´ 'cpu'
    try:
        import torch
        if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
            return "mps"
        # CUDA ìˆìœ¼ë©´ 'cuda' (ë°ìŠ¤í¬í†± í™˜ê²½ ëŒ€ë¹„)
        if torch.cuda.is_available():
            return "cuda"
    except Exception:
        pass
    return "cpu"

def load_jsonl(path: str) -> List[Dict[str, Any]]:
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            rows.append(json.loads(line))
    return rows

def to_documents(rows: List[Dict[str, Any]]) -> List[Document]:
    return [Document(page_content=r["page_content"], metadata=r["metadata"]) for r in rows]

def chunk_docs(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVER)
    return splitter.split_documents(docs)

def md5_text(s: str) -> str:
    return hashlib.md5(s.encode("utf-8")).hexdigest()

def load_cache(path: str) -> Dict[str, np.ndarray]:
    if not os.path.exists(path): return {}
    data = np.load(path, allow_pickle=True)
    keys, vecs = data["keys"].tolist(), data["vecs"]
    return {k: vecs[i] for i, k in enumerate(keys)}

def save_cache(cache: Dict[str, np.ndarray], path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    keys = np.array(list(cache.keys()), dtype=object)
    vecs = np.stack([cache[k] for k in cache.keys()], axis=0) if cache else np.zeros((0,0), dtype=np.float32)
    np.savez_compressed(path, keys=keys, vecs=vecs)

if __name__ == "__main__":
    os.environ.setdefault("TOKENIZERS_PARALLELISM", "true")
    # (ì„ íƒ) CPUì¼ ë•Œ ìŠ¤ë ˆë“œ ëŠ˜ë¦¬ê¸°
    try:
        import torch, multiprocessing
        torch.set_num_threads(max(1, multiprocessing.cpu_count() - 1))
    except Exception:
        pass

    device = autodetect_device()
    print(f"âš™ï¸ device = {device}")

    # 1) ë¡œë“œ & ì²­í¬
    raw_rows = load_jsonl(IN_JSONL)
    docs     = to_documents(raw_rows)
    chunks   = chunk_docs(docs)
    print(f"ğŸ“„ chunks: {len(chunks)}")

    # 2) ìºì‹œ ì¤€ë¹„ (chunk_md5 = md5(page_content) + ëª¨ë¸ëª…)
    cache = load_cache(CACHE_NPZ)
    need_texts, need_idx = [], []
    ids = []
    for i, d in enumerate(chunks):
        h = md5_text(d.page_content + "|" + EMB_MODEL)
        ids.append(h)
        if h not in cache:
            need_texts.append(d.page_content)
            need_idx.append(i)

    # 3) ì„ë² ë”©ê¸° (MPS/CPU ìë™, ëŒ€ë°°ì¹˜)
    embeddings = HuggingFaceEmbeddings(
        model_name=EMB_MODEL,
        model_kwargs={"device": device},
        encode_kwargs={"normalize_embeddings": NORMALIZE, "batch_size": BATCH_SIZE, "convert_to_numpy": True}
    )

    # 4) ë¯¸ë³´ìœ  ë¶„ë§Œ ë°°ì¹˜ ì¸ì½”ë”©
    if need_texts:
        print(f"ğŸ§  encode {len(need_texts)} new chunks (batch={BATCH_SIZE}) ...")
        new_vecs = embeddings.embed_documents(need_texts)  # List[List[float]] (numpy ë³€í™˜ ì˜µì…˜ ì ìš©ë¨)
        # embed_documentsê°€ numpyë¥¼ ì•ˆ ì£¼ëŠ” ë²„ì „ì´ë©´ np.arrayë¡œ ê°ì‹¸ê¸°
        new_vecs = np.array(new_vecs, dtype=np.float32)
        for idx, vec in zip(need_idx, new_vecs):
            cache[ids[idx]] = vec
        save_cache(cache, CACHE_NPZ)
        print("ğŸ’¾ cache updated.")
    else:
        print("âœ… all chunk vectors are cached.")

    # 5) ì „ì²´ ë²¡í„°/ë©”íƒ€ë°ì´í„° ì¡°ë¦½
    all_vecs = np.stack([cache[h] for h in ids], axis=0).astype(np.float32)
    metadatas = [c.metadata for c in chunks]
    texts     = [c.page_content for c in chunks]

   # --- 6) ë²¡í„°ìŠ¤í† ì–´ ìƒì„± & ì €ì¥ (ì¬ì„ë² ë”© ì—†ì´ ì§ì ‘ FAISS êµ¬ì„±) ---
    import faiss
    from langchain_community.docstore.in_memory import InMemoryDocstore
    from langchain_community.vectorstores.faiss import FAISS as LCFAISS

    dim = all_vecs.shape[1]

    # ì½”ì‚¬ì¸ ê²€ìƒ‰: ì •ê·œí™”í–ˆë‹¤ë©´ IP ì‚¬ìš©
    index = faiss.IndexFlatIP(dim) if NORMALIZE else faiss.IndexFlatL2(dim)
    index.add(all_vecs)

    # ğŸ”´ ê¼­ ë¬¸ìì—´ idë¡œ í†µì¼!
    doc_ids = [str(i) for i in range(len(texts))]

    docstore = InMemoryDocstore({
        doc_ids[i]: Document(page_content=texts[i], metadata=metadatas[i])
        for i in range(len(texts))
    })

    vs = LCFAISS(
        embedding_function=None,   # ì¬ì„ë² ë”© ì•ˆ í•¨
        index=index,
        docstore=docstore,
        index_to_docstore_id={i: doc_ids[i] for i in range(len(doc_ids))}
    )

    os.makedirs(INDEX_DIR, exist_ok=True)
    vs.save_local(INDEX_DIR)
    print(f"âœ… FAISS saved to {INDEX_DIR} with {len(texts)} chunks (dim={dim})")
