# -*- coding: utf-8 -*-
"""
[ë‚˜ë¬´ìœ„í‚¤ ì¸ë¬¼ ë¬¸ì„œ ìˆ˜ì§‘ê¸°] íŒ€ë³„ ì„ ìˆ˜ëª…ìœ¼ë¡œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì €ì¥
- CSV(íƒ€ì/íˆ¬ìˆ˜)ì—ì„œ íŒ€ í•„í„° í›„ ì„ ìˆ˜ëª… ìˆ˜ì§‘
- íŒ€ '.../ì„ ìˆ˜ë‹¨' í˜ì´ì§€ì˜ ë§í¬ë¥¼ ìš°ì„  ì‚¬ìš© (ë™ëª…ì´ì¸ ë°©ì§€ ëª©ì )
- ê·¸ë˜ë„ ë™ëª…ì´ì¸/ëª¨í˜¸ í˜ì´ì§€ë¡œ ë–¨ì–´ì§€ë©´: 'ì•¼êµ¬/KBO/íŒ€ëª…' ê¸°ì¤€ìœ¼ë¡œ í›„ë³´ë¥¼ ìˆœíšŒí•´ ì¬í•´ê²°
- ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ <íŒ€>/<ì„ ìˆ˜ëª…>.txt ë¡œ ì €ì¥

í•„ìš”: requests, beautifulsoup4
pip install requests beautifulsoup4
"""

# =========================
# Config (ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ë¨)
# =========================
team = "KT"  # 'LG', 'ë‘ì‚°', 'í•œí™”', 'KIA', 'ì‚¼ì„±', 'SSG', 'í‚¤ì›€', 'NC', 'ë¡¯ë°', 'KT' ë“±
csv_files = [
    "hitter_basic_All.csv",
    "pitcher_basic_All.csv",
]
out_root = "./namu_people_txt"  # ì €ì¥ ë£¨íŠ¸ í´ë”
use_team_page_mapping = True    # íŒ€ ì„ ìˆ˜ë‹¨ í˜ì´ì§€ ì•µì»¤ ë§µ ìš°ì„  ì‚¬ìš©
delay_base_sec = 1.0            # ìš”ì²­ ê°„ í‰ê·  ì§€ì—°(ì´ˆ)

# =========================
# ë³¸ ì½”ë“œ
# =========================
import os, re, csv, time, random
from typing import List, Dict, Tuple, Optional
from urllib.parse import urljoin, quote
import requests
from bs4 import BeautifulSoup
import difflib

BASE = "https://namu.wiki"
UA = ("Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/537.36 "
      "(KHTML, like Gecko) Chrome/120.0 Safari/537.36")

TEAM_FULL_NAME = {
    "LG": "LG íŠ¸ìœˆìŠ¤",
    "ë‘ì‚°": "ë‘ì‚° ë² ì–´ìŠ¤",
    "í•œí™”": "í•œí™” ì´ê¸€ìŠ¤",
    "KIA": "KIA íƒ€ì´ê±°ì¦ˆ",
    "ì‚¼ì„±": "ì‚¼ì„± ë¼ì´ì˜¨ì¦ˆ",
    "SSG": "SSG ëœë”ìŠ¤",
    "í‚¤ì›€": "í‚¤ì›€ íˆì–´ë¡œì¦ˆ",
    "NC": "NC ë‹¤ì´ë…¸ìŠ¤",
    "ë¡¯ë°": "ë¡¯ë° ìì´ì–¸ì¸ ",
    "KT": "KT ìœ„ì¦ˆ",
}

session = requests.Session()
session.headers.update({"User-Agent": UA, "Accept-Language": "ko,en;q=0.8"})

# -------------------- ê³µí†µ ìœ í‹¸ --------------------
def fetch_html(url: str, max_retry: int = 3, sleep=(0.7, 1.5)) -> str:
    last = None
    for _ in range(max_retry):
        resp = session.get(url, timeout=25)
        last = resp.status_code
        if resp.status_code == 200:
            return resp.text
        time.sleep(random.uniform(*sleep))
    raise RuntimeError(f"GET ì‹¤íŒ¨: {url} (status={last})")

def clean_text(s: str) -> str:
    s = re.sub(r'\u200b|\xa0|\r', ' ', s)
    s = re.sub(r'[ \t]+', ' ', s)
    s = re.sub(r'\n{3,}', '\n\n', s)
    return s.strip()

def pick_main_text(soup: BeautifulSoup) -> str:
    candidates = [
        ("article", {}),
        ("div", {"class": "wiki-article"}),
        ("div", {"id": "content"}),
        ("main", {}),
    ]
    for name, attrs in candidates:
        node = soup.find(name, attrs)
        if not node:
            continue
        for bad in node.select(".toc, .footnotes, nav, header, .ad, .advertisement"):
            bad.decompose()
        txt = node.get_text("\n", strip=True)
        if len(txt) > 200:
            return clean_text(txt)
    return clean_text(soup.get_text("\n", strip=True))

def extract_page_name(soup: BeautifulSoup, default_name="ë¬¸ì„œ") -> str:
    h1 = soup.find("h1")
    if h1 and h1.get_text(strip=True):
        return h1.get_text(strip=True)
    og = soup.find("meta", attrs={"property": "og:title"})
    if og and og.get("content"):
        return og["content"].strip()
    title = soup.find("title")
    if title and title.get_text(strip=True):
        t = re.sub(r"\s*-\s*ë‚˜ë¬´ìœ„í‚¤$", "", title.get_text(strip=True))
        return t or default_name
    return default_name

def safe_filename(name: str) -> str:
    for a,b in [("/", "ï¼"), ("\\","ï¼¼"), (":","ï¼š"), ("*","ï¼Š"),
                ("?","ï¼Ÿ"), ('"',"ï¼‚"), ("<","ï¼œ"), (">","ï¼"), ("|","ï½œ")]:
        name = name.replace(a,b)
    return (name.strip() or "ë¬¸ì„œ")

# ---------- [í—¬í¼] ì´ë¦„-ì œëª© í¬ë¡œìŠ¤ì²´í¬ ----------
def _norm(s: str) -> str:
    """ê³µë°± ì œê±° + ì†Œë¬¸ìí™” (í•œê¸€ì—” ì˜í–¥ ì—†ê³  ì˜ë¬¸/ê³µë°± ì°¨ì´ í¡ìˆ˜)"""
    return re.sub(r"\s+", "", (s or "")).lower()

def _title_has_name(name: str, page_title: str) -> bool:
    """
    í˜ì´ì§€ëª…ì— ì„ ìˆ˜ëª…ì´ 'í¬í•¨'ë˜ëŠ”ì§€ íŒë‹¨.
    ì˜ˆ: name='ì‹ ë¯¼ì¬' â†’ page_title='ì‹ ë¯¼ì¬(ì•¼êµ¬ì„ ìˆ˜)' True
        name='ì˜¤ìŠ¤í‹´' â†’ page_title='ì˜¤ìŠ¤í‹´ ë”˜' True
    """
    return _norm(name) in _norm(page_title)

# -------------------- íŒ€ í˜ì´ì§€ ë§í¬ ìˆ˜ì§‘ --------------------
def looks_like_person_loose(text: str) -> bool:
    if not text:
        return False
    # í•œê¸€/ì•ŒíŒŒë²³ì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ íŒ¨ìŠ¤ (ê´„í˜¸/ìˆ«ì í—ˆìš©)
    return bool(re.search(r"[ê°€-í£A-Za-z]", text))

def _strip_paren(s: str) -> str:
    # ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±° (ì—¬ëŸ¬ ê°œë„ ì œê±°)
    return re.sub(r"\s*\(.*?\)\s*", "", s).strip()

def _keys_from_anchor(a) -> tuple[list, str]:
    """ì•µì»¤ë¡œë¶€í„° ë§¤í•‘ í‚¤ í›„ë³´ë“¤ê³¼ ì ˆëŒ€ URLì„ ë½‘ëŠ”ë‹¤"""
    href = a.get("href") or ""
    if not href:
        return [], ""
    abs_url = urljoin(BASE, href)

    t_text = (a.get_text(strip=True) or "").strip()
    t_title = (a.get("title") or "").strip()

    cand = set()
    for s in (t_text, t_title):
        if s:
            cand.add(s)
            cand.add(_strip_paren(s))
    # ì™¸ì¸ ì„±ëª… ê³µë°± ë¶„ë¦¬(ì˜¤ìŠ¤í‹´ ë”˜ â†’ ì˜¤ìŠ¤í‹´)
    for s in list(cand):
        if " " in s:
            cand.add(s.split()[0])

    # ë…¸ì´ì¦ˆ ì œê±°: í•œê¸€/ì•ŒíŒŒë²³ ì—†ëŠ” í‚¤ ì œì™¸
    keys = [k for k in cand if re.search(r"[ê°€-í£A-Za-z]", k)]
    return keys, abs_url

def build_name_to_url_map(team_short: str) -> Dict[str, str]:
    """íŒ€ ì„ ìˆ˜ë‹¨ í˜ì´ì§€ì—ì„œ 'í‘œì‹œì´ë¦„/ë³€í˜• -> href' ë§¤í•‘(ì ˆëŒ€/ìƒëŒ€ URL ëª¨ë‘)"""
    full = TEAM_FULL_NAME.get(team_short, team_short)
    team_page = "https://namu.wiki/w/kt%20wiz/%EC%84%A0%EC%88%98%EB%8B%A8"
    html = fetch_html(team_page)
    soup = BeautifulSoup(html, "html.parser")
    mapping: Dict[str, str] = {}

    article = soup.find("article") or soup
    for a in article.select('a[href^="/w/"], a[href^="https://namu.wiki/w/"]'):
        keys, abs_url = _keys_from_anchor(a)
        if not abs_url:
            continue
        # ë¶ˆí•„ìš”í•œ ì‹œìŠ¤í…œ/í† ë¡  ë§í¬ ì œì™¸
        if any(seg in abs_url for seg in ("/discuss", "/Recent", "/ACL", "/Random", "/history")):
            continue
        for k in keys:
            if looks_like_person_loose(k):
                mapping.setdefault(k, abs_url)
    return mapping

def _fuzzy_pick_url(name: str, name2url_map: Dict[str, str]) -> tuple[str|None, str]:
    """ì •í™• ì¼ì¹˜ê°€ ì—†ìœ¼ë©´ í¼ì§€ ë§¤ì¹­ìœ¼ë¡œ URL ì„ íƒ"""
    if not name2url_map:
        return None, ""
    keys = list(name2url_map.keys())
    norm = _strip_paren(name)

    # 1) ì™„ì „ ì¼ì¹˜/ê´„í˜¸ ì œê±° ì¼ì¹˜
    for k in (name, norm):
        if k in name2url_map:
            return name2url_map[k], "team-map"

    # 2) startswith / contains ìš°ì„ 
    starts = [k for k in keys if k.startswith(name)]
    contains = [k for k in keys if name in k]
    for cand in (starts + contains):
        return name2url_map[cand], "team-mapâ‰ˆfuzzy"

    # 3) difflib ê·¼ì‚¬
    near = difflib.get_close_matches(name, keys, n=1, cutoff=0.8)
    if near:
        return name2url_map[near[0]], "team-mapâ‰ˆfuzzy"

    return None, ""

# -------------------- CSV ë¡œë”© --------------------
def load_names_from_csv_filtered(csv_paths: List[str],
                                 team_short: str,
                                 name_col: str = "ì„ ìˆ˜ëª…",
                                 team_col: str = "íŒ€ëª…") -> List[str]:
    names: List[str] = []
    for path in csv_paths:
        with open(path, "r", encoding="utf-8-sig", newline="") as f:
            reader = csv.DictReader(f)
            if name_col not in reader.fieldnames or team_col not in reader.fieldnames:
                raise ValueError(f"{path}: í•„ìš”í•œ ì»¬ëŸ¼ ì—†ìŒ. í—¤ë”={reader.fieldnames}, í•„ìš”='{name_col}','{team_col}'")
            for row in reader:
                t = (row.get(team_col) or "").strip()
                if t and team_short in t:
                    nm = (row.get(name_col) or "").strip()
                    if nm:
                        names.append(nm)
    seen, uniq = set(), []
    for n in names:
        if n not in seen:
            seen.add(n); uniq.append(n)
    return uniq

# -------------------- ë™ëª…ì´ì¸/ê²€ì¦ ë¡œì§ --------------------
def is_disambiguation_page(soup: BeautifulSoup) -> bool:
    head_txt = (soup.find("article") or soup).get_text(" ", strip=True)[:600]
    if re.search(r"ë™ìŒì´ì˜ì–´|ë™ëª…ì´ì¸", head_txt):
        return True
    if re.search(r"ë‹¤ìŒ(ê³¼|ì˜)\s+.+\s+ê°€ë¦¬í‚¬", head_txt):
        return True
    # ìƒë‹¨ ë‹¤ëŸ‰ ëª©ë¡ íœ´ë¦¬ìŠ¤í‹±
    if len((soup.select("article ul li") or [])[:12]) >= 6 and "ë¶„ë¥˜" not in head_txt:
        return True
    return False

def _article_text(soup: BeautifulSoup) -> str:
    return (soup.find("article") or soup).get_text(" ", strip=True)

def text_mentions_team(soup: BeautifulSoup, team_full: str) -> bool:
    """ë³¸ë¬¸/ì¸í¬ë°•ìŠ¤ì— íŒ€ëª…ì´ ë“±ì¥í•˜ëŠ”ì§€ ê°„ë‹¨ ê²€ì¦"""
    article = soup.find("article") or soup
    txt = article.get_text(" ", strip=True)
    return team_full in txt

def resolve_from_disambig(name: str, soup: BeautifulSoup, team_full: str) -> Optional[Tuple[str, BeautifulSoup]]:
    """ë™ëª…ì´ì¸ í˜ì´ì§€ì—ì„œ íŒ€/ì•¼êµ¬ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë¬¸ì„œ ì¬í•´ê²°"""
    article = soup.find("article") or soup

    def score_link(a) -> int:
        t = a.get_text(strip=True)
        h = a.get("href") or ""
        s = 0
        if team_full in t or team_full in h: s += 5
        if re.search(r"(ì•¼êµ¬|ì•¼êµ¬\s*ì„ ìˆ˜|KBO)", t): s += 3
        if re.search(r"%EC%95%BC%EA%B5%AC|KBO", h): s += 2
        if "(" in t and ")" in t: s += 1
        return s

    cand = []
    for a in article.select('a[href^="/w/"], a[href^="https://namu.wiki/w/"]'):
        href = a.get("href", "")
        if not href or any(seg in href for seg in ("/discuss", "/Recent", "/ACL", "/Random", "/history")):
            continue
        sc = score_link(a)
        if sc > 0:
            cand.append((sc, urljoin(BASE, href)))

    # ì¤‘ë³µ ì œê±° & ê³ ë“ì  ìš°ì„ 
    seen, uniq = set(), []
    for sc, u in sorted(cand, key=lambda x: -x[0]):
        if u not in seen:
            seen.add(u); uniq.append(u)

    # í›„ë³´ ìˆœíšŒ: íŒ€ëª… í¬í•¨ + ì œëª©ì— 'ì„ ìˆ˜ëª…' í¬í•¨ë˜ëŠ” í˜ì´ì§€ë§Œ ì±„íƒ
    for url in uniq[:10]:
        try:
            html2 = fetch_html(url)
            s2 = BeautifulSoup(html2, "html.parser")
            page_title = extract_page_name(s2, default_name=name)
            txt = (s2.find("article") or s2).get_text(" ", strip=True)
            if (team_full in txt) and _title_has_name(name, page_title):
                return url, s2
        except Exception:
            continue

    # í‘œì œ ê·œì¹™ ì¶”ì •(ë³´ì¡° ë£¨íŠ¸)
    for suffix in ["(ì•¼êµ¬ ì„ ìˆ˜)", "(ì•¼êµ¬ì„ ìˆ˜)", "(ì•¼êµ¬)", f"({team_full})"]:
        try:
            url = urljoin(BASE, "/w/" + quote(name + suffix, safe=""))
            html2 = fetch_html(url)
            s2 = BeautifulSoup(html2, "html.parser")
            page_title = extract_page_name(s2, default_name=name)
            txt = (s2.find("article") or s2).get_text(" ", strip=True)
            if (team_full in txt) and _title_has_name(name, page_title):
                return url, s2
        except Exception:
            pass
    return None

# -------------------- í•µì‹¬ íŒŒì„œ --------------------
def parse_person_by_name(name: str, name2url_map: Dict[str, str] | None, team_short: str):
    """
    1) team-map(ì •í™•/í¼ì§€)ë¡œ URL í™•ë³´ ì‹œ, ë¨¼ì € í•´ë‹¹ í˜ì´ì§€ ì œëª©ì— 'ì„ ìˆ˜ëª…' í¬í•¨ë˜ë©´ ì¦‰ì‹œ í™•ì •(í•´ê²° ì ˆëŒ€ ê¸ˆì§€)
    2) team-mapì¸ë° ì œëª©ì— ì„ ìˆ˜ëª… ë¯¸í¬í•¨ â†’ ê·¸ë•Œë§Œ ì¬ì‹œë„ (direct/resolve)
    3) ì–´ë–¤ ê²½ë¡œë“  'ìµœì¢… ì €ì¥ ì§ì „'ì—ë„ ì œëª©-ì´ë¦„ í¬ë¡œìŠ¤ì²´í¬. ë¶ˆì¼ì¹˜ë©´ ì‹¤íŒ¨ ì²˜ë¦¬
    """
    team_full = TEAM_FULL_NAME.get(team_short, team_short)

    # --- URL ì„ íƒ: team-map(í¼ì§€ í¬í•¨) â†’ ì—†ìœ¼ë©´ direct ---
    url, source = None, ""
    if name2url_map:
        # ì •í™•/í¼ì§€ ë§¤ì¹­ ì‹œë„
        url, source = _fuzzy_pick_url(name, name2url_map)
    if not url:
        url = urljoin(BASE, "/w/" + quote(name, safe=""))
        source = "direct"

    # --- ìµœì´ˆ í˜ì´ì§€ ë¡œë“œ ---
    html = fetch_html(url)
    soup = BeautifulSoup(html, "html.parser")
    page_name = extract_page_name(soup, default_name=name)

    # --- [ì¤‘ìš”] team-mapìœ¼ë¡œ ë“¤ì–´ì™”ê³ , ì œëª©ì— ì„ ìˆ˜ëª…ì´ í¬í•¨ë˜ë©´ ê·¸ëŒ€ë¡œ í™•ì • (resolve ê¸ˆì§€) ---
    if source.startswith("team-map") and _title_has_name(name, page_name):
        body_text = pick_main_text(soup)
        # ìµœì¢… ì•ˆì „ë²¨íŠ¸: ê·¸ë˜ë„ íŒ€ ë¬¸ì„œ ê°™ì€ ì—‰ëš±í•œ ê²½ìš° ë§‰ê¸° ìœ„í•´ í•œ ë²ˆ ë” í™•ì¸
        if not _title_has_name(name, page_name):
            raise RuntimeError(f"ì œëª©-ì´ë¦„ ë¶ˆì¼ì¹˜(íŒ€ë§µ í™•ì • ë‹¨ê³„): {page_name} vs {name}")
        return page_name, body_text, source  # â† ì—¬ê¸°ì„œ ë!

    # --- team-mapì¸ë° ì œëª©ì´ ì„ ìˆ˜ëª… í¬í•¨ ì•ˆ ë˜ë©´: ë³´ì¡° ë£¨íŠ¸ ì‹œë„ ---
    if source.startswith("team-map") and not _title_has_name(name, page_name):
        # 1) ë™ëª…ì´ì¸ í˜ì´ì§€ë©´ ì¬í•´ê²° ì‹œë„
        if is_disambiguation_page(soup):
            resolved = resolve_from_disambig(name, soup, team_full)
            if resolved:
                url, soup = resolved
                page_name = extract_page_name(soup, default_name=name)
                source = f"{source}->resolved"
        # 2) ê·¸ë˜ë„ ë¶ˆì•ˆí•˜ë©´ /w/<ì´ë¦„> ì§ì ‘ ì§„ì…í•´ì„œ ë‹¤ì‹œ í™•ì¸
        if not _title_has_name(name, page_name):
            try:
                url2 = urljoin(BASE, "/w/" + quote(name, safe=""))
                html2 = fetch_html(url2)
                s2 = BeautifulSoup(html2, "html.parser")
                page_name2 = extract_page_name(s2, default_name=name)
                if _title_has_name(name, page_name2):
                    soup, page_name = s2, page_name2
                    source = f"{source}->direct"
            except Exception:
                pass

    # --- directë¡œ ì¶œë°œí•œ ê²½ìš°: í•„ìš” ì‹œ ë™ëª…ì´ì¸ ì¬í•´ê²° ---
    if source == "direct" and is_disambiguation_page(soup):
        resolved = resolve_from_disambig(name, soup, team_full)
        if resolved:
            url, soup = resolved
            page_name = extract_page_name(soup, default_name=name)
            source = f"{source}->resolved"

    # --- ìµœì¢… ì¶”ì¶œ & ì €ì¥ ì „ ìµœì¢… í¬ë¡œìŠ¤ì²´í¬ ---
    body_text = pick_main_text(soup)
    if not _title_has_name(name, page_name):
        # ì—‰ëš±í•œ ë¬¸ì„œ ì €ì¥ ì°¨ë‹¨
        raise RuntimeError(f"ì œëª©-ì´ë¦„ ë¶ˆì¼ì¹˜(ìµœì¢…): {page_name} vs {name}")

    return page_name, body_text, source

# -------------------- ë©”ì¸ --------------------
def main():
    out_dir = os.path.join(out_root, team)
    os.makedirs(out_dir, exist_ok=True)

    names = load_names_from_csv_filtered(csv_files, team_short=team)
    print(f"íŒ€ '{team}' ëŒ€ìƒ ì„ ìˆ˜ëª… {len(names)}ëª…")

    name2url_map = None
    if use_team_page_mapping:
        try:
            name2url_map = build_name_to_url_map(team)
            print(f"- íŒ€ í˜ì´ì§€ ì•µì»¤ ë§¤í•‘ {len(name2url_map)}ê±´ í™•ë³´")
        except Exception as e:
            print(f"- íŒ€ í˜ì´ì§€ ë§¤í•‘ ì‹¤íŒ¨: {e}")

    failed = []
    for i, name in enumerate(names, 1):
        try:
            time.sleep(delay_base_sec * random.uniform(0.6, 1.4))
            page_name, body_text, how = parse_person_by_name(name, name2url_map, team)
            out_path = os.path.join(out_dir, safe_filename(name) + ".txt")
            with open(out_path, "w", encoding="utf-8") as f:
                f.write(body_text)
            print(f"[{i}/{len(names)}] ì €ì¥: {out_path} (ë¬¸ììˆ˜ {len(body_text):,}) â† í˜ì´ì§€ëª…: {page_name} [{how}]")
        except Exception as e:
            print(f"[{i}/{len(names)}] ì‹¤íŒ¨: {name} â†’ {e}")
            failed.append(name)

    # ë¬´ê²°ì„± ì²´í¬
    missing = [n for n in names if not os.path.exists(os.path.join(out_dir, safe_filename(n) + ".txt"))]
    print(f"\nâœ… ìˆ˜ì§‘ ì„±ê³µ: {len(names) - len(missing)} / {len(names)}")
    if failed or missing:
        union, seen = [], set()
        for n in (failed + missing):
            if n not in seen:
                seen.add(n); union.append(n)
        print("âŒ ë¯¸ìˆ˜ì§‘/ì‹¤íŒ¨ ëª©ë¡:", union)
    else:
        print("ğŸ‰ ëª¨ë“  ëŒ€ìƒì´ ì •ìƒ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
